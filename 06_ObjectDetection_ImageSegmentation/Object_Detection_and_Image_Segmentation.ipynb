{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Object Detection and Image Segmentation task**"
      ],
      "metadata": {
        "id": "W222LzQCs0u6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Object Detection**"
      ],
      "metadata": {
        "id": "rsjWYV1ms55h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download yolov3 weights\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "metadata": {
        "id": "gyrvPFtH8E1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/Corso_ML/object detection/'"
      ],
      "metadata": {
        "id": "S1HOGVt8tNmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import required packages\n",
        "import cv2\n",
        "import argparse\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "OLZQ73j38op2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output_layers(net):\n",
        "    layer_names = net.getLayerNames()\n",
        "    try:\n",
        "        output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
        "    except:\n",
        "        output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "    return output_layers\n",
        "\n",
        "\n",
        "def draw_prediction(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
        "    label = str(classes[class_id])\n",
        "    color = COLORS[class_id]\n",
        "    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
        "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)"
      ],
      "metadata": {
        "id": "DHuh4Jpv9AYD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read input image\n",
        "image = cv2.imread(path+'dog.jpg')\n",
        "\n",
        "Width = image.shape[1]\n",
        "Height = image.shape[0]\n",
        "scale = 0.00392\n",
        "\n",
        "# read class names from text file\n",
        "classes = None\n",
        "with open(path+'yolov3.txt', 'r') as f:\n",
        "    classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# generate different colors for different classes\n",
        "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
        "\n",
        "# read pre-trained model and config file\n",
        "weights = '/content/yolov3.weights'\n",
        "config =  path+'yolov3.cfg'\n",
        "print(weights)\n",
        "net = cv2.dnn.readNet(weights, config)\n",
        "\n",
        "# create input blob\n",
        "blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
        "\n",
        "# set input blob for the network\n",
        "net.setInput(blob)"
      ],
      "metadata": {
        "id": "CsSTiBFz7x9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outs = net.forward(get_output_layers(net))\n",
        "\n",
        "class_ids = []\n",
        "confidences = []\n",
        "boxes = []\n",
        "conf_threshold = 0.5\n",
        "nms_threshold = 0.4\n",
        "\n",
        "\n",
        "for out in outs:\n",
        "    for detection in out:\n",
        "        scores = detection[5:]\n",
        "        class_id = np.argmax(scores)\n",
        "        confidence = scores[class_id]\n",
        "        if confidence > 0.5:\n",
        "            center_x = int(detection[0] * Width)\n",
        "            center_y = int(detection[1] * Height)\n",
        "            w = int(detection[2] * Width)\n",
        "            h = int(detection[3] * Height)\n",
        "            x = center_x - w / 2\n",
        "            y = center_y - h / 2\n",
        "            class_ids.append(class_id)\n",
        "            confidences.append(float(confidence))\n",
        "            boxes.append([x, y, w, h])\n",
        "\n",
        "\n",
        "indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
        "\n",
        "for i in indices:\n",
        "    try:\n",
        "        box = boxes[i]\n",
        "    except:\n",
        "        i = i[0]\n",
        "        box = boxes[i]\n",
        "\n",
        "    x = box[0]\n",
        "    y = box[1]\n",
        "    w = box[2]\n",
        "    h = box[3]\n",
        "    draw_prediction(image, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n",
        "\n",
        "\n",
        "cv2.imwrite(\"object-detection.jpg\", image)"
      ],
      "metadata": {
        "id": "8XQaslY286-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Image segmentation**"
      ],
      "metadata": {
        "id": "h5SjtJLHveAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)\n",
        "# or any of these variants\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True)\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_mobilenet_v3_large', pretrained=True)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "A2TDaN84FMdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download an example image from the pytorch website\n",
        "import urllib\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/deeplab1.png\", \"deeplab1.png\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)\n"
      ],
      "metadata": {
        "id": "rPcezbnMFQbX"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# filename = path+'img3.jpg'\n",
        "input_image = Image.open(filename)\n",
        "input_image = input_image.convert(\"RGB\")\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)['out'][0]\n",
        "output_predictions = output.argmax(0)\n"
      ],
      "metadata": {
        "id": "3bwVQw8xFQU7"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a color pallette, selecting a color for each class\n",
        "palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n",
        "colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n",
        "colors = (colors % 255).numpy().astype(\"uint8\")\n",
        "\n",
        "# plot the semantic segmentation predictions of 21 classes in each color\n",
        "r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n",
        "r.putpalette(colors)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(r)"
      ],
      "metadata": {
        "id": "8APZvDwYFUXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(input_image)\n",
        "plt.title('Input Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(r, cmap='viridis')\n",
        "plt.title('Predicted Segmentation')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hxchLV-rFh9Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}